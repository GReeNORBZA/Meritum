# D07-022: Service: Tier 2 LLM integration — configurable OpenAI-compatible HTTP client, PHI stripping, response processing, hallucination guard

## Project Context
You are building **Meritum Health Technologies**, a self-serve medical billing platform for Alberta physicians (AHCIP + WCB claims). This is a monorepo with:
- `apps/api` — Fastify 5 REST API with Drizzle ORM, PostgreSQL 16
- `apps/web` — Next.js 15 frontend
- `packages/shared` — Shared Drizzle schemas, Zod validators, constants, utilities

Tech stack: TypeScript, Fastify 5, Drizzle ORM, PostgreSQL 16, Vitest + Supertest, Lucia auth.

Read `CLAUDE.md` in the project root for full conventions (naming, structure, error handling, security, testing).

## Dependencies
This task depends on: **D07-020** (condition evaluator and claim context builder).

## CRITICAL DESIGN DECISION: Configurable OpenAI-Compatible HTTP Client

The LLM integration MUST be a **configurable OpenAI-compatible HTTP client**, NOT tied to any specific model or provider. This allows the user to point it at llama.cpp server, Ollama, vLLM, or any OpenAI-compatible API (including OpenAI itself).

**Environment variables for configuration:**
- `LLM_BASE_URL` — Base URL for the LLM API (e.g., `http://localhost:8080`, `https://llm.internal.meritum.ca`, `https://api.openai.com`). No default — must be set to enable Tier 2.
- `LLM_MODEL` — Model identifier to send in the request body (e.g., `meritum-billing-7b`, `llama-3.1-8b`, `gpt-4o`). No default — must be set to enable Tier 2.
- `LLM_API_KEY` — API key sent as `Authorization: Bearer <key>`. Optional — some self-hosted servers (llama.cpp, Ollama) don't require auth.
- `LLM_TIMEOUT_MS` — Request timeout in milliseconds. Default: `3000` (from intelligence constants).

**If `LLM_BASE_URL` or `LLM_MODEL` are not set, Tier 2 is disabled entirely (graceful degradation). Log a warning at startup.**

**HTTP client implementation:**
- Use the standard `/v1/chat/completions` endpoint format: `POST {LLM_BASE_URL}/v1/chat/completions`
- Request body format:
  ```json
  {
    "model": "{LLM_MODEL}",
    "messages": [
      { "role": "system", "content": "..." },
      { "role": "user", "content": "..." }
    ],
    "temperature": 0.1,
    "max_tokens": 1024,
    "response_format": { "type": "json_object" }
  }
  ```
- Headers: `Content-Type: application/json`, `Authorization: Bearer {LLM_API_KEY}` (only if LLM_API_KEY is set)
- Use Node.js built-in `fetch` (or undici) — no external HTTP client libraries needed.
- Implement AbortController with the configured timeout.

**For tests: mock the HTTP client.** Create a `createLlmClient(config)` factory function that returns the client. In tests, inject a mock that returns predefined responses. Do NOT make real HTTP calls in tests.

## What to Build
Create `apps/api/src/domains/intel/intel.llm.ts` with:

**LLM Client Factory:**
- `createLlmClient(config?: { baseUrl?, model?, apiKey?, timeoutMs? })` — returns an LLM client object with a `chatCompletion(messages, options?)` method. Config falls back to environment variables. Mockable for tests.
- `getLlmClient()` — singleton getter. Returns null if LLM is not configured. Logs warning on first call if disabled.

**Tier 2 Analysis:**
- `analyseTier2(claimId: string, providerId: string, tier1Results: Suggestion[])` — Tier 2 LLM analysis (async):
  1. Check if LLM client is available. If not, return empty array (graceful degradation).
  2. Build anonymised claim context (PHI stripped — PHN replaced with 'XXXNNNNNN', patient name replaced with 'PATIENT').
  3. Construct structured prompt:
     - System prompt: billing domain expert instructions, constraints (never fabricate, always cite SOMB/GR, acknowledge uncertainty).
     - Context block: anonymised claim data, provider specialty, relevant SOMB rules from Reference Data, Tier 1 results.
     - Task instruction: specific analysis request.
  4. Call LLM via the OpenAI-compatible chat completions endpoint with configured timeout.
  5. Process response:
     a. Parse JSON response (explanation, confidence, source_reference).
     b. If confidence < 0.60 -> route to Tier 3 instead of Tier 2.
     c. **Hallucination guard:** validate source_reference against Reference Data. If LLM cites non-existent SOMB section or GR -> suppress suggestion, log for rule library improvement.
     d. Convert to Suggestion structure with tier = 2.
  6. If LLM timeout -> return empty array. Tier 1 suggestions already delivered synchronously.
  7. If LLM instance unavailable -> graceful fallback (no Tier 2, Tier 3 flags for complex cases).

- `stripPhi(claimContext: ClaimContext)` — replace patient PHN with placeholder, remove patient name. Keep billing structure (codes, modifiers, dates, clinical codes).

- `validateLlmSourceReference(reference: string)` — check if cited SOMB section or GR number exists in Reference Data. Return boolean.

## Security Requirements
- PHI is NEVER sent to LLM. Patient PHN, name are replaced with placeholders before prompt construction.
- Self-hosted LLM: no data leaves Meritum infrastructure. Canadian data residency maintained.
- Hallucination guard prevents displaying suggestions based on non-existent rules.
- LLM timeout prevents blocking the claim save flow.
- API key is read from environment variable, never logged or exposed in error responses.

## Expected Tests
Add tests in `apps/api/src/domains/intel/intel.test.ts` (mock the HTTP client, never make real calls):
- stripPhi replaces PHN with placeholder
- stripPhi removes patient name
- stripPhi preserves billing codes and dates
- analyseTier2 constructs correct prompt structure
- analyseTier2 respects configured timeout (via AbortController)
- analyseTier2 returns empty on timeout
- analyseTier2 routes low-confidence to Tier 3
- hallucination guard suppresses invalid SOMB reference
- hallucination guard allows valid SOMB reference
- analyseTier2 graceful fallback when LLM unavailable (LLM_BASE_URL not set)
- createLlmClient sends correct Authorization header when API key provided
- createLlmClient omits Authorization header when no API key
- createLlmClient uses configured model name in request body

## Run After Completion
After completing your changes, run:
```bash
pnpm --filter api vitest run src/domains/intel/intel.test.ts
```
Fix any errors before finishing.
