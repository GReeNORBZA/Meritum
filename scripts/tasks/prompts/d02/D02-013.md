# Task D02-013: Repository â€” admin staging operations (upload, diff, publish, rollback)

## What to Build

Add to `apps/api/src/domains/reference/reference.repository.ts`:

**Staging operations:**
- `createStagingRecord(data: { dataSet: string, uploadedBy: string, fileHash: string, recordCount: number, stagedData: unknown })` -- insert staging record with status='uploaded'. Return created record with staging_id.
- `findStagingById(stagingId: string)` -- find staging record by ID. Return full record including staged_data JSONB.
- `updateStagingStatus(stagingId: string, status: string, result?: { validation_result?: unknown, diff_result?: unknown })` -- update staging status and optionally set validation_result or diff_result. Valid transitions: uploaded -> validated, validated -> diff_generated, diff_generated -> published, any -> discarded.
- `deleteStagingRecord(stagingId: string)` -- permanently remove staging record (for discard flow).
- `listStagingByDataSet(dataSet: string)` -- list pending staging records (status != 'published' and status != 'discarded') for a data set, ordered by created_at DESC.

**Bulk data operations (for publishing):**
All bulk inserts must run within a single transaction. If any insert fails, the entire batch rolls back.

- `bulkInsertHscCodes(records: InsertHscCode[], versionId: string)` -- batch insert HSC codes. Set version_id on all records.
- `bulkInsertWcbCodes(records: InsertWcbCode[], versionId: string)` -- batch insert WCB codes.
- `bulkInsertModifiers(records: InsertModifierDefinition[], versionId: string)` -- batch insert modifier definitions.
- `bulkInsertRules(records: InsertGoverningRule[], versionId: string)` -- batch insert governing rules.
- `bulkInsertFunctionalCentres(records: InsertFunctionalCentre[], versionId: string)` -- batch insert functional centres.
- `bulkInsertDiCodes(records: InsertDiCode[], versionId: string)` -- batch insert DI codes. Must handle ~14,000 records efficiently (use chunked inserts if needed).
- `bulkInsertRrnpCommunities(records: InsertRrnpCommunity[], versionId: string)` -- batch insert RRNP communities.
- `bulkInsertPcpcmBaskets(records: InsertPcpcmBasket[], versionId: string)` -- batch insert PCPCM baskets.
- `bulkInsertExplanatoryCodes(records: InsertExplanatoryCode[], versionId: string)` -- batch insert explanatory codes.

For large bulk inserts (e.g., DI codes with ~14,000 records), chunk the inserts into batches of 1,000 records to avoid exceeding PostgreSQL parameter limits. Use `db.transaction()` to wrap all chunks.

Use Drizzle ORM query builder. Follow existing repository patterns.

## Critical Security Rules

- Staging data is isolated from live data -- staged records are stored in JSONB in the staging table, not the live tables.
- Bulk inserts must be transactional -- partial publication is not acceptable.
- File hash (SHA-256) stored for integrity verification and deduplication.

## Prerequisites

- D02-009 must be complete (database migration applied, tables exist).
- D02-010 must be complete (base repository file with version management functions).

## Tests to Write

Add tests to `apps/api/src/domains/reference/reference.test.ts`:

- createStagingRecord stores staged data
- updateStagingStatus transitions correctly
- deleteStagingRecord removes record
- bulkInsertHscCodes inserts all records in transaction
- bulkInsertHscCodes rolls back on failure
- bulkInsertDiCodes handles 14,000+ records efficiently

## FRD Reference

- Domain 2 Section 6.2: Staging workflow -- data uploaded to staging, validated, diff generated, then published or discarded.

## Run After Completion

```bash
pnpm --filter api vitest run src/domains/reference/reference.test.ts
```

All tests must pass before outputting [TASK_COMPLETE].
